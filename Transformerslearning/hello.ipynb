{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: NEGATIVE, with score: 0.9991\n",
      "label: POSITIVE, with score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp=pipeline(\"sentiment-analysis\")\n",
    "result=nlp(\"I hate you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'],4)}\")\n",
    "result=nlp(\"I love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'],4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.8546808362007141}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification,pipeline\n",
    "\n",
    "model_name=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "classifier=pipeline('sentiment-analysis',model=model,tokenizer=tokenizer)\n",
    "\n",
    "classifier(\"I love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n",
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torch\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model=AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "classes=[\"not paraphrase\",\"is paraphrase\"]\n",
    "sequence_0=\"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "paraphrase=tokenizer(sequence_0,sequence_2,return_tensors=\"pt\")\n",
    "not_paraphrase=tokenizer(sequence_0,sequence_1,return_tensors=\"pt\")\n",
    "paraphrase_classification_logits=model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits=model(**not_paraphrase).logits\n",
    "paraphrase_results=torch.softmax(paraphrase_classification_logits,dim=1).tolist()[0]\n",
    "not_paraphrase_results=torch.softmax(not_paraphrase_classification_logits,dim=1).tolist()[0]\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i]*100))}%\")\n",
    "for i in range(len(classes)):\n",
    "\tprint(f\"{classes[i]}: {int(round(not_paraphrase_results[i]*100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "\n",
    "model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "inputs=tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "batch=tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "for key,value in batch.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.0833,  4.3364],\n",
      "        [ 0.0818, -0.0418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "outputs=model(**batch)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2043e-04, 9.9978e-01],\n",
       "        [5.3086e-01, 4.6914e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "predictions=F.softmax(outputs[0],dim=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"E:/Study/Pythonlearning/Transformerslearning/hello_state1\")\n",
    "model.save_pretrained(\"E:/Study/Pythonlearning/Transformerslearning/hello_state1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at E:/Study/Pythonlearning/Transformerslearning/hello_state1 were not used when initializing DistilBertModel: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"E:/Study/Pythonlearning/Transformerslearning/hello_state1\")\n",
    "model=AutoModel.from_pretrained(\"E:/Study/Pythonlearning/Transformerslearning/hello_state1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=model(**batch,output_hidden_states=True,output_attentions=True)\n",
    "all_hidden_states,all_attentions=outputs[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig,DistilBertTokenizer,DistilBertForSequenceClassification\n",
    "config=DistilBertConfig(n_heads=8,dim=512,hidden_dim=4*512)\n",
    "tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model=DistilBertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer,DistilBertForSequenceClassification\n",
    "model_name=\"distilbert-base-uncased\"\n",
    "model=DistilBertForSequenceClassification.from_pretrained(model_name,num_labels=10)\n",
    "tokenizer=DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'titan', 'rt', '##x', 'has', '24', '##gb', 'of', 'vr', '##am']\n",
      "[101, 1037, 16537, 19387, 2595, 2038, 2484, 18259, 1997, 27830, 3286, 102]\n",
      "[CLS] a titan rtx has 24gb of vram [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "sequence=\"A Titan RTX has 24GB of VRAM\"\n",
    "tokenized_sequence=tokenizer.tokenize(sequence)\n",
    "print(tokenized_sequence)\n",
    "inputs=tokenizer(sequence)\n",
    "encoded_sequence=inputs[\"input_ids\"]\n",
    "print(encoded_sequence)\n",
    "decoded_sequence=tokenizer.decode(encoded_sequence)\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 19\n",
      "[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence_a=\"This is a short sequence.\"\n",
    "sequence_b=\"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "encoded_sequence_a=tokenizer(sequence_a)[\"input_ids\"]\n",
    "encoded_sequence_b=tokenizer(sequence_b)[\"input_ids\"]\n",
    "print(len(encoded_sequence_a),len(encoded_sequence_b))\n",
    "padded_sequence=tokenizer([sequence_a,sequence_b],padding=True)\n",
    "print(padded_sequence[\"input_ids\"])\n",
    "print(padded_sequence[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence_a=\"HuggingFace is based in NYC\"\n",
    "sequence_b=\"Where is HuggingFace based?\"\n",
    "encoded_dict=tokenizer(sequence_a,sequence_b)\n",
    "decoded=tokenizer.decode(encoded_dict[\"input_ids\"])\n",
    "print(decoded)\n",
    "print(encoded_dict['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n",
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model=AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "classed=[\"not paraphrase\",\"is paraphrase\"]\n",
    "sequence_0=\"The company HuggingFace is based in New York City\"\n",
    "sequence_1=\"Apples are especially bad for your health\"\n",
    "sequence_2=\"HuggingFace's headquarters are situated in Manhattan\"\n",
    "paraphrase=tokenizer(sequence_0,sequence_2,return_tensors=\"pt\")\n",
    "not_paraphrase=tokenizer(sequence_0,sequence_1,return_tensors=\"pt\")\n",
    "paraphrase_classification_logits=model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits=model(**not_paraphrase).logits\n",
    "paraphrase_results=torch.softmax(paraphrase_classification_logits,dim=1).tolist()[0]\n",
    "not_paraphrase_results=torch.softmax(not_paraphrase_classification_logits,dim=1).tolist()[0]\n",
    "for i in range(len(classes)):\n",
    "\tprint(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "\n",
    "for i in range(len(classes)):\n",
    "\tprint(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question', score: 0.6226, start: 34, end: 95\n",
      "Answer: 'SQuAD dataset', score: 0.5053, start: 147, end: 160\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp=pipeline(\"question-answering\")\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
    "\"\"\"\n",
    "\n",
    "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
