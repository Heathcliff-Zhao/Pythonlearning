{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caaf2db8e76c4afaa307af9ae463d878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'risk': 3891,\n",
       " 'რ': 1451,\n",
       " 'commercial': 3293,\n",
       " 'major': 2350,\n",
       " 'st': 2358,\n",
       " 'baptiste': 17329,\n",
       " '##ach': 6776,\n",
       " 'eldest': 7310,\n",
       " 'receptors': 13833,\n",
       " '##rro': 18933,\n",
       " '##宇': 30345,\n",
       " '##ith': 8939,\n",
       " 'supervisory': 26653,\n",
       " 'installation': 8272,\n",
       " 'panted': 28149,\n",
       " '##lin': 4115,\n",
       " 'chris': 3782,\n",
       " 'camping': 13215,\n",
       " '##た': 30187,\n",
       " '##an': 2319,\n",
       " 'thong': 27468,\n",
       " 'hurried': 9520,\n",
       " 'happen': 4148,\n",
       " 'angola': 13491,\n",
       " '##tor': 4263,\n",
       " 'dallas': 5759,\n",
       " '##race': 22903,\n",
       " 'outlawed': 29131,\n",
       " 'wehrmacht': 23443,\n",
       " 'remains': 3464,\n",
       " 'aimed': 6461,\n",
       " 'signs': 5751,\n",
       " '##llary': 24435,\n",
       " 'evacuated': 13377,\n",
       " 'genre': 6907,\n",
       " 'ness': 23384,\n",
       " '##ன': 29923,\n",
       " 'ri': 15544,\n",
       " '##mania': 27010,\n",
       " '##40': 12740,\n",
       " 'professional': 2658,\n",
       " 'sud': 19219,\n",
       " 'hara': 18820,\n",
       " 'possessed': 8679,\n",
       " '##held': 24850,\n",
       " 'guinea': 7102,\n",
       " '##oured': 16777,\n",
       " '##phobia': 24920,\n",
       " 'innovation': 8144,\n",
       " 'locus': 25206,\n",
       " 'yanked': 10963,\n",
       " 'hogan': 14851,\n",
       " '##parts': 26950,\n",
       " 'mw': 12464,\n",
       " '179': 20311,\n",
       " '##ך': 29797,\n",
       " '##lithic': 28508,\n",
       " 'crater': 11351,\n",
       " 'enclave': 25867,\n",
       " '$': 1002,\n",
       " '36th': 21460,\n",
       " '##ieri': 21939,\n",
       " 'estrada': 26482,\n",
       " 'philadelphia': 4407,\n",
       " 'regulator': 21618,\n",
       " '##kin': 4939,\n",
       " '##hora': 16977,\n",
       " 'bearing': 7682,\n",
       " 'excerpt': 28142,\n",
       " 'monastic': 17361,\n",
       " 'continuous': 7142,\n",
       " 'get': 2131,\n",
       " 'competes': 14190,\n",
       " 'hurricane': 7064,\n",
       " '[unused324]': 329,\n",
       " 'destroyers': 13242,\n",
       " 'sorbonne': 28452,\n",
       " 'bp': 17531,\n",
       " 'papal': 12156,\n",
       " 'paddy': 16063,\n",
       " '##chi': 5428,\n",
       " 'priced': 21125,\n",
       " 'strains': 18859,\n",
       " 'decomposition': 22511,\n",
       " 'retained': 6025,\n",
       " 'pm': 7610,\n",
       " '##dation': 20207,\n",
       " '##las': 8523,\n",
       " 'ideals': 15084,\n",
       " 'conqueror': 25466,\n",
       " 'granite': 9753,\n",
       " 'satirical': 17251,\n",
       " '##nction': 27989,\n",
       " '[unused153]': 158,\n",
       " 'defending': 6984,\n",
       " '1746': 24507,\n",
       " 'military': 2510,\n",
       " 'mythology': 11327,\n",
       " 'munster': 11348,\n",
       " 'draught': 25349,\n",
       " 'χ': 1177,\n",
       " 'grow': 4982,\n",
       " '##ization': 3989,\n",
       " 'abilities': 7590,\n",
       " '##cing': 6129,\n",
       " '##80': 17914,\n",
       " '？': 1994,\n",
       " 'congregations': 16850,\n",
       " 'pride': 6620,\n",
       " 'turtles': 16489,\n",
       " 'follower': 22399,\n",
       " '[unused409]': 414,\n",
       " 'collects': 17427,\n",
       " 'scared': 6015,\n",
       " '##tore': 19277,\n",
       " 'ottoman': 6188,\n",
       " 'futile': 24495,\n",
       " 'zheng': 20985,\n",
       " 'closed': 2701,\n",
       " 'losses': 6409,\n",
       " 'lakers': 18264,\n",
       " 'weird': 6881,\n",
       " 'cecil': 11978,\n",
       " 'fractures': 28929,\n",
       " '##bla': 28522,\n",
       " 'confused': 5457,\n",
       " 'controlling': 9756,\n",
       " 'contacted': 11925,\n",
       " 'replaces': 20736,\n",
       " 'keenan': 26334,\n",
       " 'interactive': 9123,\n",
       " 'dangling': 18737,\n",
       " 'interacting': 21935,\n",
       " '[unused34]': 35,\n",
       " 'thou': 15223,\n",
       " '##loaded': 17468,\n",
       " 'middlesbrough': 21655,\n",
       " '##將': 30354,\n",
       " 'brackets': 19719,\n",
       " 'earth': 3011,\n",
       " 'cl': 18856,\n",
       " 'listen': 4952,\n",
       " '##26': 23833,\n",
       " 'narrowly': 11866,\n",
       " '164': 17943,\n",
       " '[unused293]': 298,\n",
       " 'ᄋ': 1463,\n",
       " 'bolivia': 11645,\n",
       " 'yuri': 14331,\n",
       " 'failures': 15428,\n",
       " 'mixtape': 18713,\n",
       " 'crafted': 19275,\n",
       " 'delilah': 23006,\n",
       " 'existing': 4493,\n",
       " 'reactor': 13308,\n",
       " 'estimated': 4358,\n",
       " 'nervous': 6091,\n",
       " 'picked': 3856,\n",
       " '1550': 26245,\n",
       " 'ک': 1304,\n",
       " 'death': 2331,\n",
       " 'loaned': 13190,\n",
       " 'currents': 14731,\n",
       " 'vlad': 19163,\n",
       " '##jos': 19929,\n",
       " 'vance': 16672,\n",
       " 'pivotal': 20369,\n",
       " 'feminism': 20050,\n",
       " '##lines': 12735,\n",
       " '##ulton': 22145,\n",
       " '##vus': 27500,\n",
       " 'importance': 5197,\n",
       " 'verge': 16079,\n",
       " 'navigable': 28538,\n",
       " 'arched': 9194,\n",
       " 'ล': 1418,\n",
       " 'sap': 20066,\n",
       " 'tricked': 24929,\n",
       " '⊆': 1611,\n",
       " '神': 1925,\n",
       " 'morse': 17107,\n",
       " 'pakistani': 9889,\n",
       " 'odds': 10238,\n",
       " 'exemption': 19621,\n",
       " 'showcased': 22443,\n",
       " 'suffered': 4265,\n",
       " '##icide': 21752,\n",
       " '##naut': 24619,\n",
       " '[unused161]': 166,\n",
       " 'as': 2004,\n",
       " 'cigarettes': 15001,\n",
       " 'famine': 15625,\n",
       " 'article': 3720,\n",
       " 'hearst': 25419,\n",
       " 'oven': 17428,\n",
       " 'spirit': 4382,\n",
       " 'mansions': 26842,\n",
       " 'scissors': 25806,\n",
       " '##阝': 30496,\n",
       " '[unused556]': 561,\n",
       " '[unused331]': 336,\n",
       " 'acquired': 3734,\n",
       " '##dron': 19440,\n",
       " 'christensen': 24189,\n",
       " 'lama': 18832,\n",
       " 'competitors': 10159,\n",
       " 'hansen': 13328,\n",
       " 'ﬂ': 1985,\n",
       " 'memorable': 13432,\n",
       " 'lodges': 26767,\n",
       " '་': 1424,\n",
       " 'may': 2089,\n",
       " 'shy': 11004,\n",
       " 'disputes': 11936,\n",
       " 'elias': 14579,\n",
       " 'economical': 21791,\n",
       " 'landed': 5565,\n",
       " '##chon': 24561,\n",
       " 'detached': 12230,\n",
       " 'supreme': 4259,\n",
       " '1975': 3339,\n",
       " 'sweetheart': 12074,\n",
       " 'honeymoon': 19227,\n",
       " '[unused66]': 67,\n",
       " 'hr': 17850,\n",
       " 'bsc': 23533,\n",
       " 'k': 1047,\n",
       " 'summer': 2621,\n",
       " 'loops': 15932,\n",
       " 'switzerland': 5288,\n",
       " '##gu': 12193,\n",
       " '##if': 10128,\n",
       " '[unused934]': 939,\n",
       " 'alvarez': 16309,\n",
       " 'legitimacy': 22568,\n",
       " 'stratford': 17723,\n",
       " 'arizona': 5334,\n",
       " 'э': 1208,\n",
       " 'portland': 6734,\n",
       " 'remixes': 15193,\n",
       " 'predictions': 20932,\n",
       " 'よ': 1684,\n",
       " 'bangs': 28490,\n",
       " 'helens': 24074,\n",
       " '[unused30]': 31,\n",
       " 'appreciate': 9120,\n",
       " '##阿': 30497,\n",
       " 'gasoline': 13753,\n",
       " 'thugs': 24106,\n",
       " '##olt': 27914,\n",
       " 'theft': 11933,\n",
       " 'atop': 10234,\n",
       " 'attempted': 4692,\n",
       " 'susanna': 26681,\n",
       " 'з': 1187,\n",
       " 'iranian': 7726,\n",
       " '[unused192]': 197,\n",
       " '##曲': 30396,\n",
       " '[unused39]': 40,\n",
       " 'betty': 9306,\n",
       " 'ceo': 5766,\n",
       " 'modifications': 12719,\n",
       " 'alumni': 9441,\n",
       " 'tandem': 18231,\n",
       " 'queensland': 5322,\n",
       " 'interiors': 20769,\n",
       " 'era': 3690,\n",
       " 'cornered': 25878,\n",
       " 'english': 2394,\n",
       " '##ove': 21818,\n",
       " 'readily': 12192,\n",
       " 'differing': 16965,\n",
       " '##oney': 17791,\n",
       " 'hacking': 23707,\n",
       " 'mocked': 24195,\n",
       " '##坂': 30329,\n",
       " 'ignored': 6439,\n",
       " 'chennai': 12249,\n",
       " 'rotterdam': 15632,\n",
       " 'montagu': 26241,\n",
       " 'generals': 11593,\n",
       " 'goran': 28356,\n",
       " 'nz': 20008,\n",
       " 'symposium': 17899,\n",
       " '[unused645]': 650,\n",
       " 'sly': 18230,\n",
       " '谷': 1951,\n",
       " 'argument': 6685,\n",
       " 'insults': 23862,\n",
       " '##tula': 28970,\n",
       " '##ffe': 16020,\n",
       " 'retention': 20125,\n",
       " 'suspect': 8343,\n",
       " 'shay': 18789,\n",
       " 'maintains': 9319,\n",
       " '##are': 12069,\n",
       " '##carriage': 21539,\n",
       " 'ostensibly': 23734,\n",
       " 'taluk': 23140,\n",
       " 'watering': 25813,\n",
       " 'junior': 3502,\n",
       " 'aching': 14750,\n",
       " 'climax': 14463,\n",
       " '##ok': 6559,\n",
       " 'khz': 17737,\n",
       " 'reiterated': 28960,\n",
       " 'hayden': 13872,\n",
       " 'paula': 13723,\n",
       " 'britain': 3725,\n",
       " 'keystone': 22271,\n",
       " '##vious': 24918,\n",
       " 'ہ': 1308,\n",
       " 'disadvantaged': 27322,\n",
       " 'logging': 15899,\n",
       " 'lake': 2697,\n",
       " 'mentioning': 18625,\n",
       " 'eruptions': 28448,\n",
       " 'specific': 3563,\n",
       " 'seating': 10747,\n",
       " 'basalt': 26343,\n",
       " '##ough': 10593,\n",
       " '##iam': 25107,\n",
       " '##uring': 12228,\n",
       " '[unused60]': 61,\n",
       " 'margot': 29210,\n",
       " '区': 1782,\n",
       " 'discreet': 29321,\n",
       " 'ireland': 3163,\n",
       " '[unused428]': 433,\n",
       " 'tobacco': 9098,\n",
       " '##arable': 25236,\n",
       " 'attendance': 5270,\n",
       " 'archipelago': 13888,\n",
       " 'bow': 6812,\n",
       " 'peterborough': 17587,\n",
       " 'binoculars': 29549,\n",
       " '##к': 23925,\n",
       " '##lved': 26832,\n",
       " 'harmonies': 28594,\n",
       " 'succeeded': 4594,\n",
       " 'chromosome': 16706,\n",
       " 'cade': 18615,\n",
       " 'began': 2211,\n",
       " 'punt': 18975,\n",
       " '1935': 4437,\n",
       " '##ʒ': 29704,\n",
       " '##sov': 23230,\n",
       " 'pharmaceutical': 13859,\n",
       " 'vhs': 17550,\n",
       " '##write': 26373,\n",
       " 'lad': 14804,\n",
       " 'roll': 4897,\n",
       " 'mccarthy': 12584,\n",
       " 'duets': 29410,\n",
       " 'enthusiast': 29550,\n",
       " 'fungal': 28079,\n",
       " 'currency': 9598,\n",
       " 'prints': 11204,\n",
       " '##bery': 22509,\n",
       " 'competed': 3879,\n",
       " 'sparkling': 16619,\n",
       " 'scheduling': 19940,\n",
       " 'garfield': 20170,\n",
       " 'dude': 12043,\n",
       " '[unused808]': 813,\n",
       " '##ی': 24830,\n",
       " '279': 25745,\n",
       " 'exercising': 28428,\n",
       " 'pacific': 3534,\n",
       " 'regards': 12362,\n",
       " 'wheels': 7787,\n",
       " '##lad': 27266,\n",
       " 'securities': 12012,\n",
       " '##tadt': 18808,\n",
       " '##স': 29912,\n",
       " 'ム': 1725,\n",
       " 'directory': 14176,\n",
       " 'buzz': 12610,\n",
       " '##lion': 18964,\n",
       " 'sir': 2909,\n",
       " 'settled': 3876,\n",
       " 'paid': 3825,\n",
       " '[unused147]': 152,\n",
       " 'betrayed': 12056,\n",
       " 'occurred': 4158,\n",
       " 'skier': 21294,\n",
       " 'governor': 3099,\n",
       " 'yellowish': 17804,\n",
       " 'lighthouse': 10171,\n",
       " 'benedict': 12122,\n",
       " 'pc': 7473,\n",
       " 'ronin': 29249,\n",
       " 'handing': 13041,\n",
       " 'ports': 8831,\n",
       " 'outlet': 13307,\n",
       " 'exercised': 17747,\n",
       " 'rejects': 19164,\n",
       " 'sandy': 7525,\n",
       " '##kti': 22462,\n",
       " 'credits': 6495,\n",
       " 'tre': 29461,\n",
       " 'shootout': 18297,\n",
       " 'addition': 2804,\n",
       " 'impress': 17894,\n",
       " 'bends': 23394,\n",
       " '[unused610]': 615,\n",
       " 'mil': 23689,\n",
       " 'agrees': 10217,\n",
       " '##hood': 9021,\n",
       " 'rugged': 17638,\n",
       " '##⟩': 30156,\n",
       " 'opened': 2441,\n",
       " '37th': 23027,\n",
       " '##iling': 16281,\n",
       " 'vertical': 7471,\n",
       " '191': 19871,\n",
       " 'french': 2413,\n",
       " '##rno': 19139,\n",
       " 'monte': 10125,\n",
       " 'publish': 10172,\n",
       " 'glow': 8652,\n",
       " '##‒': 30050,\n",
       " 'legislation': 6094,\n",
       " '##ouring': 27897,\n",
       " 'leveled': 22915,\n",
       " 'heavens': 17223,\n",
       " '##stered': 24167,\n",
       " 'murderers': 28882,\n",
       " 'extremes': 28800,\n",
       " 'myles': 27056,\n",
       " 'conversion': 7584,\n",
       " 'uranium': 14247,\n",
       " 'kashmir': 13329,\n",
       " 'sodium': 13365,\n",
       " 'suggestion': 10293,\n",
       " 'separately': 10329,\n",
       " 'staggering': 26233,\n",
       " 'archive': 8756,\n",
       " '[unused421]': 426,\n",
       " 'exceptions': 11790,\n",
       " 'groaned': 9655,\n",
       " 'յ': 1230,\n",
       " 'dams': 17278,\n",
       " '##anza': 16076,\n",
       " 'successively': 24288,\n",
       " '##51': 22203,\n",
       " 'timber': 7227,\n",
       " 'misty': 15167,\n",
       " '##oys': 27153,\n",
       " 'passages': 13768,\n",
       " 'litigation': 15382,\n",
       " 'tatar': 29241,\n",
       " 'genevieve': 20245,\n",
       " 'lenses': 15072,\n",
       " '##尚': 30356,\n",
       " 'tractors': 28292,\n",
       " 'serbian': 6514,\n",
       " 'circus': 9661,\n",
       " '##used': 13901,\n",
       " '[unused992]': 997,\n",
       " 'totally': 6135,\n",
       " 'misses': 22182,\n",
       " '[unused939]': 944,\n",
       " '##11': 14526,\n",
       " 'explosives': 14792,\n",
       " '[unused299]': 304,\n",
       " 'tasked': 13487,\n",
       " 'appropriated': 29223,\n",
       " 'messaging': 24732,\n",
       " 'baccalaureate': 27802,\n",
       " 'senator': 5205,\n",
       " 'restrain': 28467,\n",
       " 'pinto': 25066,\n",
       " 'negotiating': 18875,\n",
       " 'leagues': 8121,\n",
       " 'aft': 16638,\n",
       " '##れ': 30214,\n",
       " 'orient': 16865,\n",
       " 'finished': 2736,\n",
       " '1747': 24522,\n",
       " 'flashback': 21907,\n",
       " 'couch': 6411,\n",
       " '##um': 2819,\n",
       " '##eno': 16515,\n",
       " 'hearted': 18627,\n",
       " 'tariffs': 26269,\n",
       " 'ceasefire': 26277,\n",
       " 'parasite': 21198,\n",
       " '##ك': 29835,\n",
       " 'octagonal': 22340,\n",
       " 'mines': 7134,\n",
       " 'bottom': 3953,\n",
       " 'shut': 3844,\n",
       " 'photographed': 16164,\n",
       " 'lids': 26122,\n",
       " 'goods': 5350,\n",
       " '[unused799]': 804,\n",
       " 'strikes': 9326,\n",
       " 'dmitri': 28316,\n",
       " 'francis': 4557,\n",
       " '##fire': 10273,\n",
       " 'gathering': 7215,\n",
       " '皇': 1917,\n",
       " 'share': 3745,\n",
       " 'anticipating': 26481,\n",
       " 'rubbish': 29132,\n",
       " 'indefinite': 25617,\n",
       " '##ettes': 26592,\n",
       " 'pyramid': 11918,\n",
       " 'mccain': 19186,\n",
       " '[unused564]': 569,\n",
       " 'republicans': 10643,\n",
       " 'thomas': 2726,\n",
       " 'diagrams': 26309,\n",
       " 'spielberg': 28740,\n",
       " 'cocoa': 22940,\n",
       " 'portray': 17279,\n",
       " '##lla': 4571,\n",
       " 'corrosion': 24625,\n",
       " 'subtropical': 11935,\n",
       " '##tok': 18715,\n",
       " 'analyzing': 20253,\n",
       " 'cyclones': 26069,\n",
       " '[unused738]': 743,\n",
       " 'atm': 27218,\n",
       " 'paced': 13823,\n",
       " '[unused273]': 278,\n",
       " 'jean': 3744,\n",
       " 'india': 2634,\n",
       " '1887': 6837,\n",
       " 'gavin': 9448,\n",
       " 'jumped': 5598,\n",
       " 'brownish': 19437,\n",
       " 'stu': 24646,\n",
       " 'holds': 4324,\n",
       " 'plunge': 25912,\n",
       " '##ᆼ': 30025,\n",
       " '##gram': 13113,\n",
       " 'wiping': 14612,\n",
       " '##nac': 18357,\n",
       " '[unused225]': 230,\n",
       " '##ctum': 27272,\n",
       " '⁹': 1543,\n",
       " 'honestly': 9826,\n",
       " 'swinging': 11820,\n",
       " 'upton': 26900,\n",
       " '##ilation': 29545,\n",
       " '##ities': 6447,\n",
       " 'inhaled': 15938,\n",
       " 'speaker': 5882,\n",
       " '132': 14078,\n",
       " 'empress': 10248,\n",
       " '[unused681]': 686,\n",
       " 'pools': 12679,\n",
       " '##rogated': 26565,\n",
       " 'definitions': 15182,\n",
       " 'beetles': 14538,\n",
       " '##ʁ': 29694,\n",
       " 'expected': 3517,\n",
       " 'standards': 4781,\n",
       " 'graduated': 3852,\n",
       " 'nas': 17235,\n",
       " 'convex': 18309,\n",
       " '630': 23609,\n",
       " 'simulated': 23599,\n",
       " 'thinking': 3241,\n",
       " 'davy': 23255,\n",
       " '50': 2753,\n",
       " 'i': 1045,\n",
       " 'francais': 22357,\n",
       " 'gotten': 5407,\n",
       " 'convoys': 23083,\n",
       " 'yarmouth': 28792,\n",
       " 'haji': 28174,\n",
       " 'guy': 3124,\n",
       " '##comb': 18274,\n",
       " 'besieged': 17923,\n",
       " 'lucha': 25390,\n",
       " 'leans': 12671,\n",
       " 'malmo': 23643,\n",
       " 'impatiently': 19951,\n",
       " '1780': 15051,\n",
       " '##ness': 2791,\n",
       " 'madhya': 20841,\n",
       " '[unused599]': 604,\n",
       " 'hubert': 15346,\n",
       " '##ary': 5649,\n",
       " '##rika': 23778,\n",
       " 'experiences': 6322,\n",
       " '##光': 30296,\n",
       " 'alarmed': 19260,\n",
       " '##estra': 26199,\n",
       " '##rangle': 21476,\n",
       " '[unused348]': 353,\n",
       " '##・': 30264,\n",
       " '##谷': 30477,\n",
       " 'fuzzy': 18001,\n",
       " 'opinions': 10740,\n",
       " 'auditorium': 11448,\n",
       " 'grammy': 8922,\n",
       " '##sea': 17310,\n",
       " 'tour': 2778,\n",
       " 'theatres': 13166,\n",
       " '[unused515]': 520,\n",
       " 'seater': 23392,\n",
       " 'booster': 23715,\n",
       " 'harris': 5671,\n",
       " 'storage': 5527,\n",
       " '##ase': 11022,\n",
       " 'jihad': 24815,\n",
       " '##ppet': 29519,\n",
       " '1915': 4936,\n",
       " 'chase': 5252,\n",
       " 'territorial': 7894,\n",
       " 'budget': 5166,\n",
       " 'marred': 24563,\n",
       " 'regis': 20588,\n",
       " 'devotees': 22707,\n",
       " '##チ': 30236,\n",
       " 'inlet': 15824,\n",
       " 'protesters': 13337,\n",
       " 'quakers': 28301,\n",
       " 'contexts': 18046,\n",
       " 'ones': 3924,\n",
       " 'yu': 9805,\n",
       " 'commentary': 8570,\n",
       " 'laughter': 7239,\n",
       " 'dome': 8514,\n",
       " 'mod': 16913,\n",
       " '##nett': 15361,\n",
       " 'kits': 18628,\n",
       " 'stones': 6386,\n",
       " 'invisible': 8841,\n",
       " '##ac': 6305,\n",
       " 'spacious': 22445,\n",
       " 'whatever': 3649,\n",
       " 'beams': 13110,\n",
       " 'indices': 29299,\n",
       " '##ர': 29927,\n",
       " '##train': 23654,\n",
       " 'civilizations': 24784,\n",
       " 'laval': 26205,\n",
       " 'cunning': 23626,\n",
       " 'gulp': 26546,\n",
       " '##imated': 20592,\n",
       " 'array': 9140,\n",
       " 'syllable': 16353,\n",
       " 'buffalo': 6901,\n",
       " 'rosie': 15820,\n",
       " 'spp': 26924,\n",
       " '##ful': 3993,\n",
       " 'portion': 4664,\n",
       " 'architecture': 4294,\n",
       " 'iced': 28248,\n",
       " 'durga': 28746,\n",
       " 'preferences': 18394,\n",
       " 'alias': 14593,\n",
       " '##ɪ': 29685,\n",
       " '[unused719]': 724,\n",
       " '##fs': 10343,\n",
       " 'exchanging': 25620,\n",
       " 'battleship': 17224,\n",
       " 'genie': 22519,\n",
       " '##tance': 26897,\n",
       " '[unused241]': 246,\n",
       " 'stiffened': 16090,\n",
       " 'readers': 8141,\n",
       " '⺼': 1633,\n",
       " 'bringing': 5026,\n",
       " 'alessandro': 17956,\n",
       " 'objects': 5200,\n",
       " 'heaving': 23907,\n",
       " 'travellers': 19284,\n",
       " 'henri': 8863,\n",
       " 'brant': 29182,\n",
       " 'ruined': 9868,\n",
       " '[unused940]': 945,\n",
       " 'worcester': 12539,\n",
       " 'unused': 15171,\n",
       " '##uet': 23361,\n",
       " '##going': 26966,\n",
       " '##比': 30416,\n",
       " 'hindu': 7560,\n",
       " '##ron': 4948,\n",
       " 'dispersed': 15484,\n",
       " 'engineers': 6145,\n",
       " 'invasions': 23536,\n",
       " 'half': 2431,\n",
       " '##ine': 3170,\n",
       " '°f': 8157,\n",
       " 'galileo': 21514,\n",
       " 'americana': 25988,\n",
       " '##kim': 21138,\n",
       " '##kko': 22426,\n",
       " 'math': 8785,\n",
       " 'travels': 7930,\n",
       " 'harp': 14601,\n",
       " 'fashion': 4827,\n",
       " 'academy': 2914,\n",
       " 'check': 4638,\n",
       " '##stov': 29473,\n",
       " 'dim': 11737,\n",
       " 'mara': 13955,\n",
       " 'xx': 22038,\n",
       " 'erskine': 27139,\n",
       " 'น': 1413,\n",
       " 'theirs': 17156,\n",
       " 'concacaf': 22169,\n",
       " 'bosses': 23029,\n",
       " 'encounter': 8087,\n",
       " '##有': 30399,\n",
       " 'taluka': 29512,\n",
       " 'competition': 2971,\n",
       " '##suke': 16867,\n",
       " 'elected': 2700,\n",
       " 'relationships': 6550,\n",
       " 'koreans': 24651,\n",
       " '##iidae': 15648,\n",
       " 'massey': 21402,\n",
       " 'catalyst': 16771,\n",
       " 'peptide': 25117,\n",
       " 'council': 2473,\n",
       " 'bark': 11286,\n",
       " 'throws': 11618,\n",
       " '##dent': 16454,\n",
       " 'sumo': 28193,\n",
       " 'hornet': 26795,\n",
       " 'antennae': 28624,\n",
       " 'newcastle': 8142,\n",
       " 'slacks': 27786,\n",
       " '##psis': 18409,\n",
       " '##osition': 19234,\n",
       " 'terre': 25170,\n",
       " '##ars': 11650,\n",
       " 'adjust': 14171,\n",
       " 'jd': 26219,\n",
       " 'limo': 23338,\n",
       " 'identity': 4767,\n",
       " 'saving': 7494,\n",
       " '##hen': 10222,\n",
       " 'seduction': 26962,\n",
       " '##iad': 28665,\n",
       " '##oh': 11631,\n",
       " '##ged': 5999,\n",
       " 'voice': 2376,\n",
       " 'changing': 5278,\n",
       " 'collaborated': 8678,\n",
       " '##ope': 17635,\n",
       " 'coop': 21859,\n",
       " 'layton': 23103,\n",
       " 'combined': 4117,\n",
       " 'ի': 1225,\n",
       " '102': 9402,\n",
       " '##inate': 14776,\n",
       " 'girls': 3057,\n",
       " 'teamed': 12597,\n",
       " '##ₕ': 30093,\n",
       " '##tner': 18885,\n",
       " 'alternative': 4522,\n",
       " 'express': 4671,\n",
       " '1604': 28754,\n",
       " '[unused391]': 396,\n",
       " '##aer': 27867,\n",
       " 'eric': 4388,\n",
       " 'reward': 10377,\n",
       " 'york': 2259,\n",
       " 'screw': 11224,\n",
       " 'schooner': 21567,\n",
       " 'reacted': 14831,\n",
       " 'translates': 16315,\n",
       " 'dave': 4913,\n",
       " 'straps': 19702,\n",
       " 'boyfriend': 6898,\n",
       " '440': 17422,\n",
       " '##voking': 22776,\n",
       " '1803': 12651,\n",
       " '##ま': 30203,\n",
       " '##pling': 14353,\n",
       " '##59': 28154,\n",
       " '76': 6146,\n",
       " 'sue': 9790,\n",
       " 'resin': 24604,\n",
       " 'swaying': 21826,\n",
       " 'fate': 6580,\n",
       " 'changes': 3431,\n",
       " 'fork': 9292,\n",
       " 'dumped': 14019,\n",
       " '##uve': 22909,\n",
       " '##gb': 18259,\n",
       " 'dipping': 23427,\n",
       " 'snakes': 12971,\n",
       " 'emotional': 6832,\n",
       " 'provinces': 6941,\n",
       " 'parlor': 18746,\n",
       " 'discoveries': 15636,\n",
       " 'cologne': 10918,\n",
       " 'understanding': 4824,\n",
       " 'bodyguards': 25681,\n",
       " '##nty': 29405,\n",
       " '[unused228]': 233,\n",
       " 'petersburg': 8062,\n",
       " 'lines': 3210,\n",
       " 'mora': 26821,\n",
       " '46': 4805,\n",
       " 'ल': 1334,\n",
       " 'organised': 7362,\n",
       " 'satisfied': 8510,\n",
       " 'stuffed': 11812,\n",
       " 'skyla': 21738,\n",
       " '##ending': 18537,\n",
       " '[unused477]': 482,\n",
       " '[unused537]': 542,\n",
       " 'greg': 6754,\n",
       " 'terminus': 7342,\n",
       " 'establishment': 5069,\n",
       " 'natalie': 10829,\n",
       " 'mic': 23025,\n",
       " '[unused234]': 239,\n",
       " 'send': 4604,\n",
       " 'vows': 16495,\n",
       " 'here': 2182,\n",
       " 'warriors': 6424,\n",
       " 'nursery': 13640,\n",
       " '##止': 30413,\n",
       " 'sacrifice': 8688,\n",
       " 'terminate': 20320,\n",
       " '糹': 1934,\n",
       " 'lex': 17244,\n",
       " 'anhalt': 27088,\n",
       " 'chewed': 18362,\n",
       " 'brittany': 12686,\n",
       " 'amounted': 18779,\n",
       " 'constituents': 24355,\n",
       " 'mushroom': 18565,\n",
       " 'perfectly': 6669,\n",
       " '##eving': 23559,\n",
       " 'trombone': 13914,\n",
       " 'henrietta': 20775,\n",
       " 'tigers': 7600,\n",
       " 'gwen': 11697,\n",
       " 'dominant': 7444,\n",
       " 'ninth': 6619,\n",
       " '##eded': 19082,\n",
       " 'vibe': 21209,\n",
       " '##erin': 23282,\n",
       " '##abi': 28518,\n",
       " 'dependent': 7790,\n",
       " 'frankfurt': 9780,\n",
       " 'suggested': 4081,\n",
       " '##ggle': 24679,\n",
       " '…': 1529,\n",
       " 'nord': 13926,\n",
       " 'progress': 5082,\n",
       " 'mac': 6097,\n",
       " 'lady': 3203,\n",
       " 'fender': 19028,\n",
       " 'shouting': 11273,\n",
       " '##rnet': 26573,\n",
       " 'gunfire': 16978,\n",
       " '2011': 2249,\n",
       " '##enter': 29110,\n",
       " 'seats': 4272,\n",
       " '107': 10550,\n",
       " 'saskatoon': 25447,\n",
       " 'β': 1156,\n",
       " 'colorado': 5169,\n",
       " '1916': 4947,\n",
       " 'forrest': 16319,\n",
       " '[unused545]': 550,\n",
       " 'sum': 7680,\n",
       " 'enlightenment': 16724,\n",
       " 'rankin': 25772,\n",
       " 'across': 2408,\n",
       " 'მ': 1448,\n",
       " 'casts': 23942,\n",
       " 'lack': 3768,\n",
       " 'reaction': 4668,\n",
       " 'editorial': 8368,\n",
       " 'manifold': 19726,\n",
       " 'sms': 22434,\n",
       " '##js': 22578,\n",
       " 'hydrogen': 9732,\n",
       " 'bays': 13933,\n",
       " 'persuasion': 27577,\n",
       " '##bre': 13578,\n",
       " 'macbeth': 25182,\n",
       " 'dunne': 26553,\n",
       " '1867': 7517,\n",
       " 'siegel': 27996,\n",
       " '##cies': 9243,\n",
       " 'dragged': 7944,\n",
       " 'cortex': 17132,\n",
       " 'inspired': 4427,\n",
       " 'virtue': 11870,\n",
       " 'patriotic': 14314,\n",
       " 'philips': 19087,\n",
       " 'fitness': 10516,\n",
       " 'madam': 21658,\n",
       " 'remind': 10825,\n",
       " 'freeway': 10846,\n",
       " 'windmill': 25367,\n",
       " 'wiltshire': 17045,\n",
       " 'conferred': 15186,\n",
       " 'vest': 17447,\n",
       " 'uppsala': 25720,\n",
       " 'memphis': 9774,\n",
       " '##bians': 26376,\n",
       " 'pilot': 4405,\n",
       " 'debra': 28762,\n",
       " 'stocks': 15768,\n",
       " '[unused740]': 745,\n",
       " 'town': 2237,\n",
       " '##ergy': 24395,\n",
       " 'prayed': 14283,\n",
       " 'brasil': 21133,\n",
       " '[unused979]': 984,\n",
       " '##od': 7716,\n",
       " '1952': 3999,\n",
       " '##35': 19481,\n",
       " '##aith': 22465,\n",
       " 'distressed': 24305,\n",
       " 'businessmen': 17353,\n",
       " 'mustache': 28786,\n",
       " 'toll': 9565,\n",
       " 'tempered': 22148,\n",
       " 'gifford': 29360,\n",
       " 'skaters': 24789,\n",
       " '##cation': 10719,\n",
       " 'vfl': 13480,\n",
       " 'kobe': 24113,\n",
       " 'bertram': 27515,\n",
       " 'striped': 17983,\n",
       " 'lodge': 7410,\n",
       " 'invoked': 24959,\n",
       " 'attempt': 3535,\n",
       " 'detection': 10788,\n",
       " 'yukon': 19898,\n",
       " 'det': 20010,\n",
       " 'overcame': 26463,\n",
       " '##rates': 20370,\n",
       " 'flourishing': 29571,\n",
       " 'alexandria': 10297,\n",
       " 'delicate': 10059,\n",
       " 'performer': 9256,\n",
       " 'ん': 1691,\n",
       " '244': 24194,\n",
       " '##ija': 14713,\n",
       " 'compared': 4102,\n",
       " 'kidney': 14234,\n",
       " 'stadiums': 28244,\n",
       " 'abbess': 28010,\n",
       " 'indirectly': 17351,\n",
       " 'ordained': 9492,\n",
       " 'chilled': 23362,\n",
       " '555': 29541,\n",
       " 'account': 4070,\n",
       " '##天': 30337,\n",
       " 'islamabad': 26905,\n",
       " 'encountered': 8567,\n",
       " 'faulty': 28927,\n",
       " 'notable': 3862,\n",
       " '[unused322]': 327,\n",
       " 'overheard': 20443,\n",
       " 'surround': 15161,\n",
       " '##isen': 28992,\n",
       " '学': 1817,\n",
       " '##eral': 21673,\n",
       " '##合': 30318,\n",
       " '##uer': 13094,\n",
       " 'palestine': 8976,\n",
       " 'fires': 8769,\n",
       " 'hanna': 10579,\n",
       " 'functioned': 20903,\n",
       " '[unused128]': 133,\n",
       " 'prostitution': 15016,\n",
       " 'rabbit': 10442,\n",
       " '##nosis': 27109,\n",
       " 'ל': 1253,\n",
       " '有': 1873,\n",
       " '[unused644]': 649,\n",
       " '[unused975]': 980,\n",
       " 'configured': 26928,\n",
       " 'daryl': 22514,\n",
       " 'felony': 24648,\n",
       " '##lick': 25230,\n",
       " 'sonora': 26647,\n",
       " '1992': 2826,\n",
       " 'elbows': 13690,\n",
       " 'borrow': 17781,\n",
       " 'acknowledge': 13399,\n",
       " '##post': 19894,\n",
       " 'patsy': 25382,\n",
       " 'via': 3081,\n",
       " 'tight': 4389,\n",
       " 'terminology': 18444,\n",
       " 'pronunciation': 15498,\n",
       " 'ska': 24053,\n",
       " '##tos': 13122,\n",
       " '[unused62]': 63,\n",
       " '##gonal': 20028,\n",
       " ...}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, my dog is cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"Hello, my dog is cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello, my dog is cute [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hello, my dog is cute\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.decoders.WordPiece at 0x23976138180>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 7592, 1010, 2026, 3899, 2003, 10140, 102], None, [])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.truncate_sequences(tokenizer.encode(\"Hello, my dog is cute\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, my dog is cute\", max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92e23154e7d4bc88f14303e391a7c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72e759af10747f6b6b1b6c4c27e0ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"t5-small\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 6,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Stack(\n",
       "  (embed_tokens): Embedding(32128, 512)\n",
       "  (block): ModuleList(\n",
       "    (0): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 8)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Stack(\n",
       "  (embed_tokens): Embedding(32128, 512)\n",
       "  (block): ModuleList(\n",
       "    (0): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 8)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseActDense(\n",
       "            (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32128, 512)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32128, 512)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
